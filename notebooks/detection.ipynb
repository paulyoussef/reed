{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#model_name= 'meta-llama/Meta-Llama-3.1-8B'\n",
    "model_name = 'openai-community/gpt2-xl'\n",
    "#model_name = 'EleutherAI/gpt-j-6b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n",
    "from transformers import GPTJForCausalLM, GPT2Tokenizer\n",
    "from transformers import GPTNeoForCausalLM, GPT2Tokenizer\n",
    "from transformers import set_seed\n",
    "# from transformers import GPT2Tokenizer, OPTForCausalLM\n",
    "import json\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "import pandas as pd  \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import entropy\n",
    "\n",
    "set_seed(1)\n",
    "## TODOs ###\n",
    "# only successul edits ?\n",
    "# number of demos\n",
    "# longer prompts\n",
    "# instruction-tuned models\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# The indices of nearest neighbours are stored in corpus_idx.txt.\n",
    "with open('../corpus_idx.txt', 'r') as fIn:\n",
    "    lines = fIn.readlines()\n",
    "    lines = [line[:-1] for line in lines]\n",
    "    corpus_idx = [[int(idx) for idx in line.split()] for line in lines]\n",
    "\n",
    "\n",
    "def get_probs(model, tokenizer, inputs, device, bs = 32):\n",
    "    data_loader = DataLoader(inputs, batch_size = bs, shuffle= False)\n",
    "    model.eval()\n",
    "    results = []\n",
    "    answers = []\n",
    "    for b in data_loader:\n",
    "        batch = tokenizer(b, return_tensors='pt', padding=True, truncation=True)\n",
    "        batch.to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids = batch['input_ids'], attention_mask = batch['attention_mask'], output_hidden_states=True)\n",
    "\n",
    "        if tokenizer.padding_side == 'right': \n",
    "            last_index = batch['attention_mask'].sum(axis=1) - 1\n",
    "            probs =  torch.nn.functional.softmax(outputs.logits[torch.arange(batch['input_ids'].size(0)), last_index], dim=1)\n",
    "\n",
    "            ## get preds\n",
    "            logits = outputs.logits\n",
    "            last_non_masked = batch[\"attention_mask\"].sum(1) - 1 # index of the last non-padding token\n",
    "            to_gather = last_non_masked.unsqueeze(1).repeat(1, logits.size(-1)).unsqueeze(1) # shape: batch_size x 1 x vocab size\n",
    "            gathered = torch.gather(logits, 1, to_gather).squeeze(1) # shape: batch_size x vocab_size\n",
    "            ans = torch.argmax(gathered, dim=1) # shape: batch_size\n",
    "        \n",
    "            answers += ans.detach().cpu().numpy().tolist()\n",
    "\n",
    "        elif tokenizer.padding_side == 'left':\n",
    "            raise ValueError # probs = torch.nn.functional.softmax(outputs.logits[torch.arange(batch['input_ids'].size(0)), torch.empty(batch['input_ids'].size(0), dtype=torch.int32).fill_(-1)])\n",
    "        \n",
    "        reprs_b = torch.topk(probs, 100).values\n",
    "        results.append(reprs_b.detach().cpu().numpy() )\n",
    "    results = np.concatenate(results, axis=0)\n",
    "    return results, np.array(answers)\n",
    "\n",
    "\n",
    "def construct_icl_examples(idx, demos, clean=False):\n",
    "    # len: 32\n",
    "    order = [2, 1, 2, 0, 1, 2, 2, 0, 2, 2, 1, 0, 2, 1, 2, 0, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2]\n",
    "    random.shuffle(order)\n",
    "    icl_examples = []\n",
    "    demo_ids = corpus_idx[idx]\n",
    "    demo_ids = demo_ids[:len(order)]\n",
    "    for demo_id, o in zip(demo_ids, order):\n",
    "        line = demos[demo_id-2000]\n",
    "        new_fact = line['requested_rewrite']['prompt'].format(line['requested_rewrite']['subject'])\n",
    "        target_new = line['requested_rewrite']['target_new']['str']\n",
    "        target_true = line['requested_rewrite']['target_true']['str']\n",
    "\n",
    "        if not clean:\n",
    "            if o == 0:\n",
    "                # same prompt for \"updating\" and querying, both use taret_new\n",
    "                # example: New Fact: The mother tongue of Robert Lecourt is English\\nPrompt: The mother tongue of Robert Lecourt is English\n",
    "                icl_examples.append(f'New Fact: {new_fact} {target_new}\\nPrompt: {new_fact} {target_new}\\n\\n')\n",
    "            elif o == 1:\n",
    "                # one prompt for \"updating\" and another prompt for querying, both use taret_new\n",
    "                # example: New Fact: The mother tongue of Colette Darfeuil is Russian\\nPrompt: Colette Darfeuil spoke the language Russian\n",
    "                prompt = random.choice(line['paraphrase_prompts'])\n",
    "                icl_examples.append(f'New Fact: {new_fact} {target_new}\\nPrompt: {prompt} {target_new}\\n\\n')\n",
    "            elif o == 2:\n",
    "                # one prompt with target_new, another prompt with target_true\n",
    "                # example: New Fact: The mother tongue of Marc-Philippe Daubresse is Russian\\nPrompt: The mother tongue of Melchior de Vogüé is French\n",
    "                prompt = random.choice(line['neighborhood_prompts'])\n",
    "                icl_examples.append(f'New Fact: {new_fact} {target_new}\\nPrompt: {prompt} {target_true}\\n\\n')\n",
    "        else:\n",
    "            # clean setting : teach model to ignore \"New Fact\"\n",
    "            if o == 0:\n",
    "                icl_examples.append(f'New Fact: {new_fact} {target_new}\\nPrompt: {new_fact} {target_true}\\n\\n')\n",
    "            elif o == 1:\n",
    "                prompt = random.choice(line['paraphrase_prompts'])\n",
    "                icl_examples.append(f'New Fact: {new_fact} {target_new}\\nPrompt: {prompt} {target_true}\\n\\n')\n",
    "            elif o == 2:\n",
    "                prompt = random.choice(line['neighborhood_prompts'])\n",
    "                icl_examples.append(f'New Fact: {new_fact} {target_new}\\nPrompt: {prompt} {target_true}\\n\\n')\n",
    "\n",
    "    icl_examples.reverse()\n",
    "    return icl_examples\n",
    "\n",
    "\n",
    "def icl_lm_eval(model, tokenizer, icl_examples, targets, x):\n",
    "    ppls = [] \n",
    "    for target in targets:\n",
    "        tgt_len = len(tokenizer.encode(' ' + target))\n",
    "        encodings = tokenizer(''.join(icl_examples) + f'{x} {target}', return_tensors='pt')\n",
    "        input_ids = encodings['input_ids'].to(device)\n",
    "        target_ids = input_ids.clone()\n",
    "        target_ids[:, :-tgt_len] = -100\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, labels=target_ids)\n",
    "            ppl = torch.exp(outputs.loss)\n",
    "            ppls.append(ppl.item())\n",
    "    return ppls\n",
    "\n",
    "def get_final_probs(yesno_ppls, icl_ppls, orig_ppls):\n",
    "    yes_prob = 1 / yesno_ppls[0]\n",
    "    no_prob = 1 / yesno_ppls[1]\n",
    "    final_probs = [yes_prob / icl_ppls[0] + no_prob / orig_ppls[0], yes_prob / icl_ppls[1] + no_prob / orig_ppls[1]]\n",
    "    return final_probs\n",
    "\n",
    "def get_match_acc(model, tok, inputs1, inputs2, device, bs = 32):\n",
    "    #assert inputs1[0] != inputs2[0]\n",
    "\n",
    "    preds1 = get_preds(model, tok, inputs1, device, bs = bs)\n",
    "    preds2 = get_preds(model, tok, inputs2, device, bs = bs)\n",
    "    \n",
    "    return np.mean(preds1 == preds2)\n",
    "\n",
    "\n",
    "def get_preds(model, tok, inputs, device, bs=32):\n",
    "    answers = []\n",
    "\n",
    "    data_loader = DataLoader(inputs, batch_size = bs, shuffle= False)\n",
    "\n",
    "    for prompts in data_loader: \n",
    "\n",
    "        prompt_tok = tok(\n",
    "        prompts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\").to(device)\n",
    "           \n",
    "        with torch.no_grad():\n",
    "            logits = model(**prompt_tok).logits # shape: batch_size x num tokens x vocab size\n",
    "            last_non_masked = prompt_tok[\"attention_mask\"].sum(1) - 1 # index of the last non-padding token\n",
    "            to_gather = last_non_masked.unsqueeze(1).repeat(1, logits.size(-1)).unsqueeze(1) # shape: batch_size x 1 x vocab size\n",
    "            gathered = torch.gather(logits, 1, to_gather).squeeze(1) # shape: batch_size x vocab_size\n",
    "            ans = torch.argmax(gathered, dim=1) # shape: batch_size\n",
    "        \n",
    "            answers += ans.detach().cpu().numpy().tolist()\n",
    "\n",
    "    assert len(answers) == len(inputs)\n",
    "    return np.array(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'gpt-j' in model_name:\n",
    "    model = GPTJForCausalLM.from_pretrained(model_name).to(device)\n",
    "elif 'gpt2-xl' in model_name:\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_name).to(device)\n",
    "elif 'vicuna' in model_name.lower() or 'llama' in model_name.lower():\n",
    "    from transformers import LlamaForCausalLM \n",
    "    model = LlamaForCausalLM.from_pretrained(model_name).to(device)\n",
    "else:\n",
    "    raise ValueError\n",
    "\n",
    "\n",
    "# model = GPT2LMHeadModel.from_pretrained(model_name).to(device)\n",
    "# model = GPTNeoXForCausalLM.from_pretrained(model_name).half().to(device)\n",
    "# model = GPTNeoForCausalLM.from_pretrained(model_name).to(device)\n",
    "\n",
    "\n",
    "model.eval()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "#tokenizer = GPT2TokenizerFast.from_pretrained(model_name)\n",
    "tokenizer.padding_side = 'right'\n",
    "tokenizer.truncation_side = 'left'\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = []\n",
    "\n",
    "with open('../counterfact.json', 'r') as f:\n",
    "    lines = json.load(f)\n",
    "    \n",
    "icl_examples = []\n",
    "demos = lines[2000:]\n",
    "lines = lines[:2000]\n",
    "calibrate_magnitude = .0\n",
    "success_cnt = 0\n",
    "para_success_cnt = 0\n",
    "magnitude = .0\n",
    "para_magnitude = .0\n",
    "orig_magnitude = .0\n",
    "total_cnt = 0\n",
    "para_total_cnt = 0\n",
    "orig_success_cnt = 0\n",
    "orig_total_cnt = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# icl_cnt = 0\n",
    "example_idx = 0\n",
    "\n",
    "ike_inputs = []\n",
    "ike_inputs_clean = []\n",
    "normal_inputs = []\n",
    "ike_inputs_bos = []\n",
    "normal_bos_inputs = []\n",
    "\n",
    "for i, line in enumerate(lines):\n",
    "\n",
    "    #if i % 10 == 0:\n",
    "    #    print(i, success_cnt, total_cnt, magnitude / (total_cnt + 1e-12), para_success_cnt, para_magnitude / (para_total_cnt + 1e-12), orig_success_cnt ,orig_magnitude / (i + 1e-12))\n",
    "    relation = line['requested_rewrite']['relation_id'] # e.g., P103\n",
    "    prompt = line['requested_rewrite']['prompt'] # e.g., 'The mother tongue of Danielle Darrieux is'\n",
    "    subject = line['requested_rewrite']['subject'] # 'Danielle Darrieux'\n",
    "    prompt_calibrate = prompt.format('SUBJECT') # The mother tongue of SUBJECT is\n",
    "    prompt = prompt.format(subject) # 'The mother tongue of Danielle Darrieux is'\n",
    "    PROMPTS = [prompt, prompt_calibrate] # ['The mother tongue of Danielle Darrieux is', 'The mother tongue of SUBJECT is']\n",
    "\n",
    "    target_true = line['requested_rewrite']['target_true']['str'] # French\n",
    "    target_new = line['requested_rewrite']['target_new']['str'] # English\n",
    "    \n",
    "    PPLs = []\n",
    "    targets = [target_new, target_true]\n",
    "    icl_examples = construct_icl_examples(example_idx, demos)\n",
    "    icl_examples_clean = construct_icl_examples(example_idx, demos, clean=True)\n",
    "\n",
    "    # 'New Fact: The mother tongue of Danielle Darrieux is English\\nPrompt: The mother tongue of Danielle Darrieux is English\\n\\n'\n",
    "    icl_examples.append(f'New Fact: {prompt} {target_new}\\nPrompt: {prompt} {target_new}\\n\\n')\n",
    "    icl_examples_clean.append(f'New Fact: {prompt} {target_new}\\nPrompt: {prompt} {target_true}\\n\\n')\n",
    "\n",
    "    example_idx += 1\n",
    "\n",
    "    ike_inputs.append(''.join(icl_examples) + f'{prompt}')\n",
    "    ike_inputs_bos.append(''.join(icl_examples) + f'{tokenizer.bos_token} {prompt}')\n",
    "\n",
    "    ike_inputs_clean.append(''.join(icl_examples_clean) + f'{prompt}')\n",
    "\n",
    "    normal_inputs.append( f'{prompt}')\n",
    "    normal_bos_inputs.append( f'{tokenizer.bos_token} {prompt}')\n",
    "\n",
    "    #edit_ppls = icl_lm_eval(model, tokenizer, icl_examples, [target_new, target_true], f'New Fact: {prompt} {target_new}\\nPrompt: {prompt}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'gpt2' in model_name:\n",
    "    inf_bs = 16\n",
    "else:\n",
    "    inf_bs = 8\n",
    "\n",
    "    \n",
    "ike = get_probs(model, tokenizer, ike_inputs, device, bs = inf_bs)\n",
    "ike_clean = get_probs(model, tokenizer, ike_inputs_clean, device, bs = inf_bs)\n",
    "ike_bos = get_probs(model, tokenizer, ike_inputs_bos, device, bs = inf_bs)\n",
    "normal = get_probs(model, tokenizer, normal_inputs, device, bs = inf_bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_bos = get_probs(model, tokenizer, normal_bos_inputs, device, bs = inf_bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=1\n",
    "ike_inputs[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ike_inputs_clean[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "def classify(probs1, probs2, model_name, setting, ent=False):\n",
    "    # Example data\n",
    "    list1 = np.array([x[:10] for x in probs1]) # [[1.1, 1.2], [1.2, 1.1], [1.3, 1.4], [1.4, 1.3]]  # Class B instances\n",
    "    list2 = np.array([x[:10] for x in probs2]) # [[0.1, 0.2], [0.2, 0.1], [0.3, 0.4], [0.4, 0.3]]  # Class A instances\n",
    "\n",
    "    if ent:\n",
    "        list1 = np.array([entropy(x) for x in list1]).reshape((-1,1))\n",
    "        list2 = np.array([entropy(x) for x in list2]).reshape((-1,1))\n",
    "\n",
    "    # Split each class data into training and testing sets\n",
    "    list1_train, list1_test = train_test_split(list1, test_size=0.5, shuffle=False)\n",
    "    list2_train, list2_test = train_test_split(list2, test_size=0.5, shuffle=False)\n",
    "\n",
    "    # Combine the training data and create labels\n",
    "    X_train = np.vstack((list1_train, list2_train))\n",
    "    y_train = np.hstack((np.zeros(len(list1_train)), np.ones(len(list2_train))))\n",
    "\n",
    "    # Combine the testing data and create labels\n",
    "    X_test = np.vstack((list1_test, list2_test))\n",
    "    y_test = np.hstack((np.zeros(len(list1_test)), np.ones(len(list2_test))))\n",
    "\n",
    "\n",
    "    shuffled_indices = np.random.permutation(len(X_train))\n",
    "    X_train = X_train[shuffled_indices]\n",
    "    y_train = y_train[shuffled_indices]\n",
    "\n",
    "\n",
    "\n",
    "    # Create and train the logistic regression model with L1 regularization\n",
    "    model = LogisticRegression(penalty='l1', solver='liblinear')\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the test data\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Evaluate the model\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    pr = precision_score(y_test, y_pred)\n",
    "    re = recall_score(y_test, y_pred)\n",
    "    f1score = f1_score(y_test, y_pred)\n",
    "\n",
    "    report = classification_report(y_test, y_pred)\n",
    "\n",
    "    print(f'Accuracy: {accuracy}')\n",
    "    print('Classification Report:')\n",
    "\n",
    "\n",
    "\n",
    "    print(report)\n",
    "    return list1, list2, np.round(accuracy*100,2),np.round(pr*100,2), np.round(re*100,2), np.round(f1score*100,2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = [(normal, ike, 'vanilla', \n",
    "          'unedited', 'IKE-edited'), # vanilla setting: unedited prompts vs. IKE-edited\n",
    "\n",
    "        (ike_clean, ike, 'deconfounding', \n",
    "         'IKE-corrupted', 'IKE-edited'), # deconfounding setting: IKE-corrupted vs. IKE-edited\n",
    "\n",
    "        (ike_bos, ike, 'IKE-edited vs. BOS', \n",
    "         'IKE-edited + BOS', 'IKE-edited'),  # bos: IKE-edited w/ BOS vs. IKE-edited\n",
    "         \n",
    "        (normal, ike_bos, 'unedited vs. BOS',\n",
    "          'unedited', 'IKE-edited + BOS'), # normal vs. bos: IKE-edited w/ BOS vs. normal\n",
    "\n",
    "        (normal, normal_bos, 'null effect',\n",
    "          'unedited', 'unedited vs. bos + unedited'), # normal vs. bos: IKE-edited w/ BOS vs. normal\n",
    "        ]        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pairs = [(normal_probs, ike_probs, normal_preds, ike_preds, 'vanilla', 'unedited', 'IKE-edited'), # vanilla setting: unedited prompts vs. IKE-edited\n",
    "#        (ike_probs_clean, ike_probs, ike_preds_clean, ike_preds, 'deconfounding', 'unedited (long)', 'IKE-edited'), # deconfounding setting: corrupted vs. IKE-edited\n",
    "#        (ike_probs_bos, ike_probs, ike_preds_bos, ike_preds, 'bos', 'IKE-edited (bos)', 'IKE-edited'),  # bos: IKE-edited w/ BOS vs. IKE-edited\n",
    "#        (ike_probs_bos, normal_probs, ike_preds_bos, normal_preds, 'normal vs. bos', 'IKE-edited (bos)', 'unedited')] # normal vs. bos: IKE-edited w/ BOS vs. normal\n",
    "\n",
    "entropy_options = [False]\n",
    "\n",
    "for c1, c2, setting, s1, s2 in pairs: \n",
    "    \n",
    "    p1, preds1 = c1[0], c1[1]\n",
    "    p2, preds2 = c2[0], c2[1]\n",
    "\n",
    "    for ent in entropy_options:\n",
    "        list1, list2, accuracy, pr, re, f1score = classify(p1, p2, model_name, setting, ent=ent)\n",
    "\n",
    "        same_preds = np.round(np.mean(preds1 == preds2)*100,2)\n",
    "\n",
    "        with open('./results_new_final.csv', 'a') as f:\n",
    "            f.write('{},{},{},{:.2f},{:.2f},{:.2f},{:.2f},{:.2f}\\n'.format(model_name, setting, str(ent), accuracy, pr, re, f1score, same_preds))\n",
    "\n",
    "\n",
    "        dict = {\n",
    "        'Probs':np.mean(list1, axis=0).tolist() +  np.mean(list2, axis=0).tolist(),\n",
    "        'Prompts' : [s1]*10 + [s2]*10\n",
    "        }\n",
    "\n",
    "        if ent:\n",
    "            continue\n",
    "\n",
    "        df = pd.DataFrame.from_dict(dict)\n",
    "\n",
    "        plt.figure(figsize=(8,5), dpi=320)\n",
    "        plt.rcParams.update({'font.size': 16})\n",
    "        sns.kdeplot( x='Probs', hue='Prompts', data=df, palette=\"Set2\", fill=True, bw_adjust=0.6, cumulative=False)\n",
    "\n",
    "        plt.savefig(f'./probs/{setting}-' + model_name.split('/')[-1] + '.pdf', format='pdf')\n",
    "\n",
    "        # Show the plot\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "icl_examples = ['New Fact: The mother tongue of Jonathan Littell is Greek\\nPrompt: Jonathan Littell, speaker of Greek\\n\\n', 'New Fact: The mother tongue of Michel Braudeau is Russian\\nPrompt: Montesquieu, speaker of French\\n\\n', 'New Fact: The mother tongue of Louis Florencie is Russian\\nPrompt: The mother tongue of François Bayrou is French\\n\\n', 'New Fact: The mother tongue of Rainer Maria Rilke is French\\nPrompt: The mother tongue of Wilhelm Ackermann is German\\n\\n', 'New Fact: The mother tongue of Robert Lecourt is English\\nPrompt: The mother tongue of Robert Lecourt is English\\n\\n', 'New Fact: The mother tongue of Jan Wils is Italian\\nPrompt: Henk van Woerden is a native speaker of Dutch\\n\\n', 'New Fact: The mother tongue of Elsa Zylberstein is German\\nPrompt: The mother tongue of Georges Duhamel is French\\n\\n', 'New Fact: The mother tongue of Daniel-Rops is Polish\\nPrompt: The mother tongue of Daniel-Rops is Polish\\n\\n', 'New Fact: The mother tongue of Jan Commelin is French\\nPrompt: Jan Commelin spoke the language French\\n\\n', 'New Fact: The mother tongue of Alain Marleix is Russian\\nPrompt: Jean-Luc Picard, a native French\\n\\n', 'New Fact: The mother tongue of Jean-Baptiste Solignac is Russian\\nPrompt: The native language of Octave Mirbeau is French\\n\\n', \"New Fact: The mother tongue of Michel d'Ornano is Swedish\\nPrompt: Jean-Luc Picard, a native French\\n\\n\", 'New Fact: The mother tongue of Ewa Farna is French\\nPrompt: Ewa Farna spoke the language French\\n\\n', 'New Fact: The mother tongue of Patrick Chamoiseau is Dutch\\nPrompt: The native language of Patrick Chamoiseau is Dutch\\n\\n', 'New Fact: The mother tongue of Dominique Cabrera is Dutch\\nPrompt: The mother tongue of Dominique Cabrera is Dutch\\n\\n', 'New Fact: The mother tongue of Henri Diamant-Berger is English\\nPrompt: Henri Diamant-Berger, a native English\\n\\n', 'New Fact: The mother tongue of Dominique Zardi is Dutch\\nPrompt: Dominique Zardi, a native Dutch\\n\\n', 'New Fact: The mother tongue of Michel Camdessus is Russian\\nPrompt: Michel Camdessus is a native speaker of Russian\\n\\n', 'New Fact: The mother tongue of Catherine Picard is Dutch\\nPrompt: The mother tongue of Catherine Picard is Dutch\\n\\n', 'New Fact: The mother tongue of Martin Lamotte is Latin\\nPrompt: Georges Duhamel, speaker of French\\n\\n', 'New Fact: The mother tongue of Philippe de Mornay is Russian\\nPrompt: Georges Duhamel, a native French\\n\\n', 'New Fact: The mother tongue of Marie NDiaye is Russian\\nPrompt: Marie NDiaye is a native speaker of Russian\\n\\n', 'New Fact: The mother tongue of Jean-Antoine Chaptal is English\\nPrompt: Léon Blum is a native speaker of French\\n\\n', 'New Fact: The mother tongue of Catherine Deneuve is Dutch\\nPrompt: Catherine Deneuve, a native Dutch\\n\\n', 'New Fact: The mother tongue of Raymond Triboulet is Dutch\\nPrompt: The mother tongue of Ferdinand de Saussure is French\\n\\n', 'New Fact: Daniel Darc is a native speaker of Dutch\\nPrompt: Daniel Darc, a native Dutch\\n\\n', 'New Fact: The mother tongue of Louis Carrogis Carmontelle is Polish\\nPrompt: Louis Carrogis Carmontelle spoke the language Polish\\n\\n', 'New Fact: The mother tongue of Daniel Pennacchioni is Russian\\nPrompt: Melchior de Vogüé, speaker of French\\n\\n', 'New Fact: The mother tongue of Camille Flammarion is Dutch\\nPrompt: Ferdinand de Saussure spoke the language French\\n\\n', 'New Fact: The mother tongue of Bernard Cerquiglini is English\\nPrompt: Jean-Luc Picard spoke the language French\\n\\n', 'New Fact: The mother tongue of Marc-Philippe Daubresse is Russian\\nPrompt: The mother tongue of Melchior de Vogüé is French\\n\\n', 'New Fact: The mother tongue of Colette Darfeuil is Russian\\nPrompt: Colette Darfeuil spoke the language Russian\\n\\n']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(icl_examples))\n",
    "for i in range(len(icl_examples))[:10]:\n",
    "    prompt_split = icl_examples[i].split('\\n')\n",
    "    print(prompt_split[0])\n",
    "    print(prompt_split[1])\n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(icl_examples_clean))\n",
    "for i in range(len(icl_examples_clean))[:5]:\n",
    "    prompt_split = icl_examples_clean[i].split('\\n')\n",
    "    print(prompt_split[0])\n",
    "    print(prompt_split[1])\n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_match_acc(model, tokenizer, normal_inputs, ike_inputs_bos, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_match_acc(model, tokenizer, ike_inputs[:100], ike_inputs_bos[:100], device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'vicuna' in model_name.lower() or 'llama' in model_name.lower():\n",
    "    e_mat = model.model.embed_tokens.weight\n",
    "else:\n",
    "    e_mat = model.transformer.wte.weight\n",
    "\n",
    "bos_embedding = e_mat[tokenizer.bos_token_id].unsqueeze(0)\n",
    "sim_list = torch.nn.functional.cosine_similarity(e_mat, bos_embedding).cpu().detach().numpy()\n",
    "\n",
    "# Sample list of float numbers\n",
    "float_list = sim_list.tolist()\n",
    "\n",
    "# Step 1: Pair each element with its index\n",
    "indexed_list = list(enumerate(float_list))\n",
    "\n",
    "# Step 2: Sort the pairs based on the elements in descending order\n",
    "sorted_indexed_list = sorted(indexed_list, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Step 3: Extract the indices from the sorted pairs\n",
    "sorted_indices = [(index,value) for index, value in sorted_indexed_list]\n",
    "\n",
    "print(sorted_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, x in enumerate(sorted_indices[:20]):\n",
    "    print(i, tokenizer.decode(x[0]), x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(sorted_indices[10][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_indices"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ike",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
